{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d1a137",
   "metadata": {},
   "source": [
    "## QJE Tesseract Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# for OCR using PyTesseract\n",
    "import cv2                              # pre-processing images\n",
    "import pytesseract                      # extracting text from images\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt         # displaying output images\n",
    "from PIL import Image\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04eb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_x = 3.0 # horizontal zoom\n",
    "zoom_y = 3.0 # vertical zoom\n",
    "mat = fitz.Matrix(zoom_x, zoom_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='D:\\\\docs\\\\Masters\\\\Data\\\\QJE_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52792592",
   "metadata": {},
   "outputs": [],
   "source": [
    "investigate=Merged[(Merged.year<=2010)&(Merged.year>=2000)]\n",
    "for i in investigate.index:\n",
    "    print(Merged.iloc[i]['stable_url'])\n",
    "    if pd.isna(Merged.iloc[i]['affiliations'])==True:\n",
    "        print(Merged.iloc[i]['Jstor_authors'])\n",
    "        print(Merged.iloc[i]['Jstor_title'])\n",
    "        print(Merged.iloc[i]['content_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in investigate.index:\n",
    "    filepath=path+'\\\\'+investigate.iloc[i]['stable_url'].split('/')[-1]+'.pdf'\n",
    "    if os.path.exists(filepath):\n",
    "        doc=fitz.open(filepath)\n",
    "        print(investigate.iloc[i]['year'])\n",
    "        print(investigate.iloc[i]['issue'])\n",
    "        print(investigate.iloc[i]['volume'])\n",
    "        print(investigate.iloc[i]['Jstor_authors'])\n",
    "        print(investigate.iloc[i]['Jstor_title'])\n",
    "        for page in doc:\n",
    "            #if (page.number == 1) or (page.number==(doc.page_count-1)):\n",
    "            png = path+\"\\\\\" + investigate.iloc[i]['stable_url'].split('/')[-1].split('.')[0] + '_page-%i.png' % page.number\n",
    "            print(png)\n",
    "            #if os.path.exists(png)==True:\n",
    "            pix = page.get_pixmap(matrix=mat)\n",
    "            pix.save(png)\n",
    "            #parsed[page.number]=[]\n",
    "\n",
    "            original_image = cv2.imread(png)\n",
    "\n",
    "            # convert the image to grayscale\n",
    "            gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            plt.figure(figsize=(25, 15))\n",
    "            plt.imshow(gray_image, cmap='gray')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def converter(teststring):\n",
    "    teststring=teststring.replace('.','[.]')\n",
    "    print(teststring)\n",
    "    #for i in range(len(teststring)):\n",
    "    #    if teststring[i] == '.':\n",
    "    #        teststring=teststring[0:i-1]+'.'+teststring[i:]\n",
    "    #teststring=teststring.replace('I','.')\n",
    "            \n",
    "    for i in teststring:\n",
    "        if (i not in string.ascii_lowercase) & (i not in string.ascii_uppercase) & (i !=' ') & (i!='.') & (i!='[') & (i!=']'):\n",
    "            teststring=teststring.replace(i,'.')\n",
    "    return teststring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumberofPages(text):\n",
    "    if pd.isna(text)==False:\n",
    "        if re.search('\\d',text):\n",
    "            temp=text.split(',')\n",
    "            pages=0\n",
    "            print(temp)\n",
    "            for m in temp:\n",
    "                if '-' in m:\n",
    "                    t=str(m).split('-')\n",
    "                    pages=pages+int(re.sub('\\D','',t[1]))-int(re.sub('\\D','',t[0]))+1\n",
    "                else:\n",
    "                    pages+=1\n",
    "            return pages\n",
    "    return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_refs(SCANNED_FILE, mat, path, k_val, keyword):\n",
    "    doc = fitz.open(SCANNED_FILE)\n",
    "    parsed={}\n",
    "    references={}\n",
    "    found=0\n",
    "    for page in reversed(doc):\n",
    "        if (page.number >= 1):\n",
    "            png = path+\"\\\\\" + SCANNED_FILE.split('\\\\')[-1].split('.')[0] + '_page-%i.png' % page.number\n",
    "            if os.path.exists(png)==False:\n",
    "                pix = page.get_pixmap(matrix=mat)\n",
    "                print(png)\n",
    "                pix.save(png)\n",
    "\n",
    "            parsed[page.number]=[]\n",
    "            references[page.number]=[]\n",
    "\n",
    "            original_image = cv2.imread(png)\n",
    "            # convert the image to grayscale\n",
    "            gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            #plt.figure(figsize=(25, 15))\n",
    "            #plt.imshow(gray_image, cmap='gray')\n",
    "            #plt.show()\n",
    "\n",
    "            # Performing OTSU threshold\n",
    "            ret, threshold_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n",
    "\n",
    "            #plt.figure(figsize=(25, 15))\n",
    "            #plt.imshow(threshold_image, cmap='gray')\n",
    "            #plt.show()\n",
    "\n",
    "            rectangular_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (k_val, k_val))\n",
    "\n",
    "            # Applying dilation on the threshold image\n",
    "            dilated_image = cv2.dilate(threshold_image, rectangular_kernel, iterations = 1)\n",
    "\n",
    "            #plt.figure(figsize=(25, 15))\n",
    "            #plt.imshow(dilated_image)\n",
    "            #plt.show()\n",
    "\n",
    "            # Finding contours\n",
    "            contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # Creating a copy of the image\n",
    "            copied_image = original_image.copy()\n",
    "\n",
    "            mask = np.zeros(original_image.shape, np.uint8)\n",
    "            #i=1\n",
    "            # Looping through the identified contours\n",
    "            # Then rectangular part is cropped and passed on to pytesseract\n",
    "            # pytesseract extracts the text inside each contours\n",
    "            # Extracted text is then written into a text file\n",
    "            for cnt in contours:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                # Cropping the text block for giving input to OCR\n",
    "                cropped = copied_image[y:y + h, x:x + w]\n",
    "                # Apply OCR on the cropped image\n",
    "                text = pytesseract.image_to_string(cropped, lang='lat', config='--oem 3 --psm 4')\n",
    "                #print(i)\n",
    "                print(text)\n",
    "                parsed[page.number].append(text)\n",
    "                print(regex.search(keyword,text.upper()))\n",
    "                if regex.search(keyword, text.upper()) is not None:\n",
    "                    print('found')\n",
    "                    return {'found': parsed}\n",
    "                #masked = cv2.drawContours(mask, [cnt], 0, (255, 255, 255), -1)\n",
    "                #i=i+1\n",
    "    return {'raw': parsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89460424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pngs(SCANNED_FILE, mat, path, k_val, keyword, ignore):\n",
    "    doc = fitz.open(SCANNED_FILE)\n",
    "    parsed={}\n",
    "    references={}\n",
    "    found=0\n",
    "    for page in reversed(doc):\n",
    "        if (page.number >= 1):\n",
    "            png = path+\"\\\\\" + SCANNED_FILE.split('\\\\')[-1].split('.')[0] + '_page-%i.png' % page.number\n",
    "            if os.path.exists(png)==False:\n",
    "                pix = page.get_pixmap(matrix=mat)\n",
    "                print(png)\n",
    "                pix.save(png)\n",
    "\n",
    "            parsed[page.number]=[]\n",
    "            references[page.number]=[]\n",
    "\n",
    "            original_image = cv2.imread(png)\n",
    "            # convert the image to grayscale\n",
    "            gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            #plt.figure(figsize=(25, 15))\n",
    "            #plt.imshow(gray_image, cmap='gray')\n",
    "            #plt.show()\n",
    "\n",
    "            # Performing OTSU threshold\n",
    "            ret, threshold_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n",
    "\n",
    "            #plt.figure(figsize=(25, 15))\n",
    "            #plt.imshow(threshold_image, cmap='gray')\n",
    "            #plt.show()\n",
    "\n",
    "            rectangular_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (k_val, k_val))\n",
    "\n",
    "            # Applying dilation on the threshold image\n",
    "            dilated_image = cv2.dilate(threshold_image, rectangular_kernel, iterations = 1)\n",
    "\n",
    "            #plt.figure(figsize=(25, 15))\n",
    "            #plt.imshow(dilated_image)\n",
    "            #plt.show()\n",
    "\n",
    "            # Finding contours\n",
    "            contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # Creating a copy of the image\n",
    "            copied_image = original_image.copy()\n",
    "\n",
    "            mask = np.zeros(original_image.shape, np.uint8)\n",
    "            #i=1\n",
    "            # Looping through the identified contours\n",
    "            # Then rectangular part is cropped and passed on to pytesseract\n",
    "            # pytesseract extracts the text inside each contours\n",
    "            # Extracted text is then written into a text file\n",
    "            for cnt in contours:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                # Cropping the text block for giving input to OCR\n",
    "                cropped = copied_image[y:y + h, x:x + w]\n",
    "                # Apply OCR on the cropped image\n",
    "                text = pytesseract.image_to_string(cropped, lang='lat', config='--oem 3 --psm 4')\n",
    "                #print(i) \n",
    "                #print(regex.search(keyword,text.upper()))\n",
    "                if text=='':\n",
    "                    #print('empty af')\n",
    "                    continue\n",
    "                \n",
    "                parsed[page.number].append(text)    \n",
    "                \n",
    "                if regex.search(ignore, text.upper()) is not None:\n",
    "                    print('000110000')\n",
    "                    #print(regex.search(ignore, text.upper()))\n",
    "                    #print(text)\n",
    "                    continue\n",
    "                \n",
    "                if regex.search('[a-zA-Z]', text.upper()) is None:\n",
    "                    print('00020000')\n",
    "                    #print(text)\n",
    "                    continue\n",
    "                    \n",
    "                print(text)\n",
    "                \n",
    "                if found ==1:\n",
    "                    print('found')\n",
    "                    if regex.search('[a-zA-Z]', text.upper()) is not None:\n",
    "                        \n",
    "                        return {'found':text,'raw': parsed}\n",
    "                if regex.search(keyword, text.upper()) is not None:\n",
    "                    print('found')\n",
    "                    found=1\n",
    "                \n",
    "                #masked = cv2.drawContours(mask, [cnt], 0, (255, 255, 255), -1)\n",
    "                #i=i+1\n",
    "    return {'raw': parsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SCANNED_FILE=path+'\\\\40961018.pdf'\n",
    "SCANNED_FILE=path+'\\\\25098840.pdf'\n",
    "SCANNED_FILE=path+'\\\\27867508.pdf'\n",
    "#SCANNED_FILE=path+'\\\\2586940.pdf'\n",
    "#SCANNED_FILE=path+'\\\\2586895.pdf'\n",
    "#SCANNED_FILE=path+'\\\\2696451.pdf'\n",
    "SCANNED_FILE=path+'\\\\40961014.pdf'\n",
    "SCANNED_FILE=path+'\\\\2951257.pdf'\n",
    "t0=time.time()\n",
    "ignore='((ARE GHETTOS GOOD OR BAD?){e<=3}(\\n| \\d)|^(THIS CONTENT DOWNLOADED){e<=3}|^(QUARTERLY JOURNAL OF ECONOMICS){e<=3}\\n|^\\d)'\n",
    "affiliations=generate_pngs(SCANNED_FILE, mat, path, 30, '(^|\\n)(REFERENCES){e<=3}(\\n| )', ignore)\n",
    "t1=time.time()\n",
    "total=t1-t0\n",
    "print(total)\n",
    "affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time.time()\n",
    "\n",
    "for i in Merged[(Merged['year']>=1977) & (Merged['year']<=1979) & (Merged['content_type']!='MISC') & (Merged['content_type']!='Review')].index:\n",
    "    if Merged.iloc[i]['Jstor_authors'] is not NaN: \n",
    "        if \"Suggested by\" not in Merged.iloc[i]['Jstor_authors']:\n",
    "            authors=str(Merged.iloc[i]['Jstor_authors']).replace(' and ',', ').replace(\"  \",' ').split(',')\n",
    "            filepath=path+'\\\\'+Merged.iloc[i]['stable_url'].split('/')[-1]+'.pdf'\n",
    "            if os.path.exists(filepath)==True:\n",
    "                print(filepath)\n",
    "                print(Merged.iloc[i]['year'])\n",
    "                print(authors)\n",
    "                ignore='(('+Merged.iloc[i]['Jstor_title'].upper()+'){e<=3}(\\n| \\d| )|^(THIS CONTENT DOWNLOADED){e<=3}|^(QUARTERLY JOURNAL OF ECONOMICS){e<=3}\\n|^\\d)'\n",
    "                affiliations=generate_pngs(filepath, mat, path, 38, '(^|\\n)(REFERENCES){e<=3}(\\n| )', ignore)                \n",
    "                print(affiliations)\n",
    "                dict[Merged.iloc[i]['stable_url'].split('/')[-1]]={'affiliations': affiliations, \n",
    "                                                                   'content_type': Merged.iloc[i]['content_type'], \n",
    "                                                                   'authors': authors, \n",
    "                                                                   'stable_url': Merged.iloc[i]['stable_url']}\n",
    "            else:\n",
    "                dict[Merged.iloc[i]['stable_url'].split('/')[-1]]='PDF not available, download at '+ Merged.iloc[i]['stable_url']\n",
    "t1=time.time()\n",
    "total=t1-t0\n",
    "print(total)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adacbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(path+'//QJE_aff_ref_output_1977_1989.json','w') as fp:\n",
    "    json.dump(dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c00c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for l in dict.keys():\n",
    "    if 'found' in dict[l]['affiliations'].keys():\n",
    "        count+=1\n",
    "    else:\n",
    "        print(l)\n",
    "print(len(dict))\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
