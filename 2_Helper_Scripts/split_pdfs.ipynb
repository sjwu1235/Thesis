{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617e2f26",
   "metadata": {},
   "source": [
    "# Splitting Pdfs\n",
    "\n",
    "The following code takes a pdf from jstor, checks for the coverpage, and if the coverpage exists, it creates a pdf without the coverage with certain naming conventions as specified. This is to account for documents that were taken from scihub and do not have a jstor coverpage.\n",
    "\n",
    "Next, each pdf is split into different pages with page 1 indicated by suffix 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36e200a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import fitz #for opening pdfs\n",
    "\n",
    "# for OCR using PyTesseract\n",
    "import cv2                              # pre-processing images\n",
    "import pytesseract                      # extracting text from images\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt         # displaying output images\n",
    "from PIL import Image\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2ae47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOURNALS= ['AER', 'JPE', 'ECTA', 'RES', 'QJE']\n",
    "#read in all processed masterlists\n",
    "All=pd.DataFrame()\n",
    "for i in JOURNALS:\n",
    "    All=pd.concat([pd.read_excel('C:\\\\Users\\\\sjwu1\\\\Journal_Data\\\\datadumps\\\\combined\\\\'+i+'_M_sco_du.xlsx'),All], ignore_index=True)\n",
    "\n",
    "#Create a batch file\n",
    "All=All[All.duplicated()==False].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53c32c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60354, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4749af",
   "metadata": {},
   "source": [
    "##  Small correction\n",
    "Change ECONOMETRICA to ECTA for ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38f00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "All['id']=All['stable_url'].str.split('/').str[-1]\n",
    "All.loc[All['Jstor_journal']==\"ECONOMETRICA\",'Jstor_journal']='ECTA'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897312b",
   "metadata": {},
   "source": [
    "## Reduce the set to exclude reviews and miscellaneous content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd8d211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([     'stable_url',   'Jstor_authors',     'Jstor_title',\n",
       "        'Jstor_abstract',    'content_type',       'issue_url',\n",
       "                 'pages',            'year',          'volume',\n",
       "                 'issue',   'Jstor_journal',            'type',\n",
       "                       0,  'scopus_authors',    'scopus_title',\n",
       "        'scopus_journal',             'DOI',    'affiliations',\n",
       "       'scopus_abstract',       'citations',   'document type',\n",
       "        'index keywords', 'author keywords',      'Unnamed: 0',\n",
       "         'document_type',       'footnotes',             'raw',\n",
       "            'references',              'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "368a2f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Article', 'Comment', 'Reply', 'MISC', 'Rejoinder', 'Review',\n",
       "       'Discussion', 'Review2'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(All.content_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed126806",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reduced=All[['Jstor_authors', 'year', 'Jstor_journal', 'Jstor_title','volume', 'issue','content_type', 'stable_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6da461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_1940=Reduced[(Reduced['Jstor_authors'].isnull()==False) & (Reduced['year']>=1940)] #only select those papers that are 1940s or later\n",
    "R_1940=R_1940.reset_index().drop('index', axis=1) #reset index\n",
    "R_1940.loc[:, 'authors_caps'] = R_1940['Jstor_authors'].copy().str.upper().to_numpy() #make field to capitalize all author names\n",
    "R_1940_NM=R_1940[R_1940['content_type']!='MISC'].reset_index().drop('index', axis=1) #exclude miscellaneous\n",
    "R_1940_NMR=R_1940_NM[R_1940_NM['content_type']!='Review'].reset_index().drop('index', axis=1) #exclude reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d616c",
   "metadata": {},
   "source": [
    "## Set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a4f1936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Article' 'Comment' 'Reply' 'Rejoinder' 'Discussion']\n",
      "['QJE' 'RES' 'ECTA' 'JPE' 'AER']\n"
     ]
    }
   ],
   "source": [
    "path='D:\\\\docs\\\\Masters\\\\Data\\\\' #path to data\n",
    "Merged=R_1940_NMR\n",
    "print(Merged['content_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dcf674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "redsets=R_1940_NM[(R_1940_NM['content_type']!='Review')&(R_1940_NM['year']<=2010)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c069ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_x = 2.0 # horizontal zoom\n",
    "zoom_y = 2.0 # vertical zoom\n",
    "mat = fitz.Matrix(zoom_x, zoom_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739cab93",
   "metadata": {},
   "source": [
    "## Set functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9815bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a pdf in doc2_name taken from doc1 from the page specified inclusive\n",
    "def make_new_pdf(doc1, doc2_name, from_pg):\n",
    "    doc2 = fitz.open()                 # new empty PDF\n",
    "    doc2.insert_pdf(doc1, from_page = from_pg)  \n",
    "    pg_count=doc2.page_count\n",
    "    doc2.save(doc2_name)\n",
    "    doc2.close()\n",
    "    return pg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a fitz object doc1 it will insert the specified pages inclusive into the name specified in doc2_name\n",
    "def make_new_pdf2(doc1, doc2_name, from_pg, to_pg):\n",
    "    doc2 = fitz.open()                 # new empty PDF\n",
    "    doc2.insert_pdf(doc1, from_page = from_pg, to_page = to_pg)  # first 10 pages\n",
    "    pg_count=doc2.page_count\n",
    "    doc2.save(doc2_name)\n",
    "    doc2.close()\n",
    "    return pg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cb4d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cover page removal\n",
    "def coverpage_removal(SCANNED_FILE, path):\n",
    "    doc = None\n",
    "    page= None\n",
    "    try:\n",
    "        doc=fitz.open(SCANNED_FILE)\n",
    "        page=doc[0]\n",
    "    except:\n",
    "        doc.close()\n",
    "        raise Exception(\"this file is corrupt\")\n",
    "    \n",
    "    png = path + SCANNED_FILE.split('\\\\')[-1].split('.')[0] + '_page-%i.png' % page.number\n",
    "    if os.path.exists(png)==False:\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        #print(png)\n",
    "        pix.save(png)\n",
    "    \n",
    "    doc2_name=path+SCANNED_FILE.split('\\\\')[-1].split('.')[0]+'_wo_cover.pdf'\n",
    "    if os.path.exists(doc2_name)==False:\n",
    "        original_image = cv2.imread(png)\n",
    "\n",
    "        text = pytesseract.image_to_string(original_image, lang='lat', config='--oem 3 --psm 6')\n",
    "\n",
    "        #print(doc2_name)\n",
    "        if (re.search('AUTHOR\\(S\\)', text.upper()) is not None) or (re.search('PUBLISHED BY:', text.upper()) is not None):\n",
    "            make_new_pdf(doc, doc2_name, 1)\n",
    "            print('found')\n",
    "        else:\n",
    "            make_new_pdf(doc, doc2_name, 0)\n",
    "            print('not found')\n",
    "    doc.close()\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f84776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function splits the pdf into pages and saves them to the given path and returns a list of the pdf paths\n",
    "def shard(SCANNED_FILE, path, year):\n",
    "    df=pd.DataFrame()\n",
    "    doc = None\n",
    "    try:\n",
    "        doc=fitz.open(SCANNED_FILE)\n",
    "    except:\n",
    "        doc.close()\n",
    "        raise Exception(\"this file is corrupt\")\n",
    "        \n",
    "    for page in doc:\n",
    "        doc2_name=path+SCANNED_FILE.split('\\\\')[-1].split('.')[0]+'_page-%i.pdf' % page.number\n",
    "        if os.path.exists(doc2_name)==False:\n",
    "            make_new_pdf2(doc, doc2_name, page.number, page.number)\n",
    "        df=pd.concat([df, pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                'pdf_url': '\\''+bucket+SCANNED_FILE.split('\\\\')[-1].split('.')[0]+'_page-%i.pdf' % page.number+'\\'',\n",
    "                'year': year\n",
    "            }\n",
    "        ]\n",
    "        )], ignore_index=True)\n",
    "    doc.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d499796",
   "metadata": {},
   "source": [
    "## Going through the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pandas objects to store the list of new pdf paths\n",
    "JPE_refs=pd.DataFrame()\n",
    "ECTA_refs=pd.DataFrame()\n",
    "QJE_refs=pd.DataFrame()\n",
    "AER_refs=pd.DataFrame()\n",
    "RES_refs=pd.DataFrame()\n",
    "\n",
    "ranges={\n",
    "    #\"AER\": [1940, 2010, AER_refs], # I usually run these one at a time\n",
    "   # \"ECTA\": [1940, 2010, ECTA_refs],\n",
    "    #\"JPE\": [1940, 2010, JPE_refs],\n",
    "   # \"QJE\": [1940, 2010, QJE_refs],\n",
    "    \"RES\": [1940, 1950, RES_refs]\n",
    "}\n",
    "for journal in ranges.keys():\n",
    "    bucket='https://myawsbucket-1231.s3.eu-west-3.amazonaws.com/'+journal+'_shards/'\n",
    "    ret_frame=None\n",
    "\n",
    "    filter=Merged[(Merged['year']<=ranges[journal][1]) & (Merged['year']>=ranges[journal][0])& (Merged['Jstor_journal']==journal)]\n",
    "    for i in filter.index:\n",
    "        # make the file paths\n",
    "        path2=path+Merged.iloc[i]['Jstor_journal']+'_data\\\\'\n",
    "        #filepath=path2+Merged.iloc[i]['stable_url'].split('/')[-1]+'.pdf'\n",
    "        filepath2=path2+'wo_cover\\\\'Merged.iloc[i]['stable_url'].split('/')[-1]+'_wo_cover.pdf'\n",
    "        \n",
    "        print(filepath2)\n",
    "        if os.path.exists(filepath2)==True:\n",
    "            #coverpage_removal(filepath, path2) #remove cover page\n",
    "            ret_frame=shard(filepath2, path2, Merged.iloc[i]['year']) #shard\n",
    "            ranges[journal][2]=pd.concat([ranges[journal][2], ret_frame], ignore_index=True) #concat to the pandas objects\n",
    "        else:\n",
    "            print(filepath+' missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922e57c",
   "metadata": {},
   "source": [
    "## Save the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges['AER'][2].to_csv(path+\"\\\\aer_refs_all.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges['ECTA'][2].to_csv(path+\"\\\\ecta_refs_all.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfbc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges['JPE'][2].to_csv(path+\"\\\\jpe_refs_all.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42938b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges['QJE'][2].to_csv(path+\"\\\\qje_refs_all.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d49bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges['RES'][2].to_csv(path+\"\\\\res_refs_all.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
