{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dfbcd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt         # displaying output images\n",
    "import cv2 \n",
    "import openai\n",
    "import json\n",
    "import regex\n",
    "import tiktoken\n",
    "import os\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734146f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"/Users/sijiawu/Work/Refs Danae/Thesis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c4546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429a831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    #gpt-3.5-turbo-16k\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    r_out = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        request_timeout=1000\n",
    "    )\n",
    "    return r_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa75b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"/Users/sijiawu/Work/Refs Danae/Thesis/Data\"\n",
    "temp=base_path+'/PDFs/RES/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ffabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged=pd.read_excel(base_path+'/Combined/RES_M_sco_du.xlsx')\n",
    "Merged.loc[Merged['journal']==\"The Review of Economic Studies\",'journal']='RES'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec2d4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged[\"ID\"]=Merged[\"URL\"].str.split(\"/\").str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6d310c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt=\"/Users/sijiawu/Work/Refs Danae/Thesis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e959c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1=pt+\"/Data\\RES_refs_output_2011_2020.json\"\n",
    "res_2=pt+\"/Data\\RES_refs_output_2001_2010.json\"\n",
    "res_3=pt+\"/Data\\RES_refs_output_1991_2000.json\"\n",
    "res_4=pt+\"/Data\\RES_refs_output_1981_1990.json\"\n",
    "res_5=pt+\"/Data\\RES_refs_output_1971_1980.json\"\n",
    "res_6=pt+\"/Data\\RES_refs_output_1961_1970.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dec84aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "res=[\n",
    "#     res_1,\n",
    "#       res_2,\n",
    "      res_3,\n",
    "#       res_4,\n",
    "#       res_5,\n",
    "#       res_6\n",
    "     ]\n",
    "for file in res:\n",
    "    with open(file) as f:\n",
    "        temp_data = json.load(f)\n",
    "        data=data|temp_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada755f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f568a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    #gpt-3.5-turbo-16k\n",
    "    #gpt-3.5-turbo-0613\n",
    "#   \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "  try:\n",
    "      encoding = tiktoken.encoding_for_model(model)\n",
    "  except KeyError:\n",
    "      encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "  if model == \"gpt-3.5-turbo-0613\":  # note: future models may deviate from this\n",
    "      num_tokens = 0\n",
    "      for message in messages:\n",
    "          num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "          for key, value in message.items():\n",
    "              num_tokens += len(encoding.encode(value))\n",
    "              if key == \"name\":  # if there's a name, the role is omitted\n",
    "                  num_tokens += -1  # role is always required and always 1 token\n",
    "      num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "      return num_tokens\n",
    "  else:\n",
    "      raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "  See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54918e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "b2b836bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.833333333333336"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.keys())*300/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "459913dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "948d0688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "d21ce207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indivs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "cd9de53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(apps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ec0e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 to parse\n",
      "current time:- 2023-11-03 02:41:58.949497\n",
      "1221 too long. Using big context model.\n",
      "340 2297945 completed\n",
      "66.5255069732666\n",
      "1221\n",
      "current time:- 2023-11-03 02:43:05.480987\n",
      "1687 too long. Using big context model.\n",
      "341 2297946 completed\n",
      "89.79598093032837\n",
      "1687\n",
      "current time:- 2023-11-03 02:44:35.295468\n",
      "1337 too long. Using big context model.\n",
      "342 2297947 completed\n",
      "79.86744499206543\n",
      "1337\n",
      "current time:- 2023-11-03 02:45:55.159993\n",
      "1618 too long. Using big context model.\n",
      "343 2297948 completed\n",
      "100.07755899429321\n",
      "1618\n",
      "current time:- 2023-11-03 02:47:35.249026\n",
      "1312 too long. Using big context model.\n",
      "344 2297949 completed\n",
      "69.18835616111755\n",
      "1312\n",
      "current time:- 2023-11-03 02:48:44.443604\n",
      "1199 too long. Using big context model.\n",
      "345 2297950 completed\n",
      "60.087459087371826\n",
      "1199\n",
      "current time:- 2023-11-03 02:49:44.536503\n",
      "1041 standard\n",
      "346 2297825 completed\n",
      "49.90513825416565\n",
      "1041\n",
      "current time:- 2023-11-03 02:50:34.477898\n",
      "737 standard\n",
      "347 2297826 completed\n",
      "34.47965598106384\n",
      "737\n",
      "current time:- 2023-11-03 02:51:08.937811\n",
      "1398 too long. Using big context model.\n",
      "348 2297827 completed\n",
      "71.09957790374756\n",
      "1398\n",
      "current time:- 2023-11-03 02:52:20.047030\n",
      "2320 too long. Using big context model.\n",
      "349 2297828 completed\n",
      "129.30882096290588\n",
      "2320\n",
      "current time:- 2023-11-03 02:54:29.364015\n",
      "2076 too long. Using big context model.\n",
      "350 2297829 completed\n",
      "120.55566096305847\n",
      "2076\n",
      "current time:- 2023-11-03 02:56:29.932426\n",
      "1414 too long. Using big context model.\n",
      "351 2297830 completed\n",
      "105.88565397262573\n",
      "1414\n",
      "Regex error: 2297831\n",
      "current time:- 2023-11-03 02:58:15.830562\n",
      "948 standard\n",
      "353 2297832 completed\n",
      "45.75827598571777\n",
      "948\n",
      "current time:- 2023-11-03 02:59:01.592587\n",
      "743 standard\n",
      "354 2297833 completed\n",
      "33.46011209487915\n",
      "743\n",
      "current time:- 2023-11-03 02:59:35.057760\n",
      "662 standard\n",
      "355 2297834 completed\n",
      "32.13648581504822\n",
      "662\n",
      "current time:- 2023-11-03 03:00:07.199321\n",
      "1018 standard\n",
      "356 2297835 completed\n",
      "52.84136390686035\n",
      "1018\n",
      "current time:- 2023-11-03 03:01:00.045698\n",
      "181 standard\n",
      "357 2297836 completed\n",
      "7.67626690864563\n",
      "181\n",
      "2298003 has no references, check it\n",
      "1991\n",
      "current time:- 2023-11-03 03:01:07.740381\n",
      "3019 too long. Using big context model.\n",
      "359 2298004 completed\n",
      "197.0578989982605\n",
      "3019\n",
      "current time:- 2023-11-03 03:04:24.810015\n",
      "1723 too long. Using big context model.\n",
      "360 2298005 completed\n",
      "90.96913981437683\n",
      "1723\n",
      "current time:- 2023-11-03 03:05:55.791669\n",
      "2086 too long. Using big context model.\n",
      "361 2298006 completed\n",
      "169.1444718837738\n",
      "2086\n",
      "current time:- 2023-11-03 03:08:44.951135\n",
      "2508 too long. Using big context model.\n",
      "362 2298007 completed\n",
      "162.57823777198792\n",
      "2508\n",
      "current time:- 2023-11-03 03:11:27.543216\n",
      "1732 too long. Using big context model.\n",
      "363 2298008 completed\n",
      "94.69411706924438\n",
      "1732\n",
      "current time:- 2023-11-03 03:13:02.242345\n",
      "1194 too long. Using big context model.\n",
      "364 2298009 completed\n",
      "69.97493696212769\n",
      "1194\n",
      "current time:- 2023-11-03 03:14:12.225344\n",
      "1892 too long. Using big context model.\n",
      "365 2298010 completed\n",
      "108.4943699836731\n",
      "1892\n",
      "current time:- 2023-11-03 03:16:00.736876\n",
      "1858 too long. Using big context model.\n",
      "366 2298011 completed\n",
      "114.90068101882935\n",
      "1858\n",
      "current time:- 2023-11-03 03:17:55.660085\n",
      "2298012 this has an abnormally long reference list at 3687, process separately\n",
      "current time:- 2023-11-03 03:17:55.667725\n",
      "2298013 this has an abnormally long reference list at 3964, process separately\n",
      "current time:- 2023-11-03 03:17:55.671246\n",
      "2226 too long. Using big context model.\n",
      "369 2298014 completed\n",
      "123.33694005012512\n",
      "2226\n",
      "current time:- 2023-11-03 03:19:59.026348\n",
      "1748 too long. Using big context model.\n",
      "370 2297964 completed\n",
      "97.09835195541382\n",
      "1748\n",
      "current time:- 2023-11-03 03:21:36.137174\n",
      "1357 too long. Using big context model.\n",
      "371 2297965 completed\n",
      "81.5463330745697\n",
      "1357\n",
      "current time:- 2023-11-03 03:22:57.688063\n",
      "1026 standard\n",
      "372 2297966 completed\n",
      "59.1154100894928\n",
      "1026\n",
      "current time:- 2023-11-03 03:23:56.809062\n",
      "731 standard\n",
      "373 2297967 completed\n",
      "31.42485022544861\n",
      "731\n",
      "current time:- 2023-11-03 03:24:28.240672\n",
      "1291 too long. Using big context model.\n",
      "374 2297968 completed\n",
      "69.01723790168762\n",
      "1291\n",
      "current time:- 2023-11-03 03:25:37.273381\n",
      "1016 standard\n",
      "375 2297969 completed\n",
      "53.57649302482605\n",
      "1016\n",
      "current time:- 2023-11-03 03:26:30.852734\n",
      "1108 too long. Using big context model.\n",
      "376 2297970 completed\n",
      "61.070029973983765\n",
      "1108\n",
      "current time:- 2023-11-03 03:27:31.936782\n",
      "2520 too long. Using big context model.\n",
      "377 2297971 completed\n",
      "151.33255290985107\n",
      "2520\n",
      "current time:- 2023-11-03 03:30:03.279972\n",
      "840 standard\n",
      "378 2297972 completed\n",
      "49.44283580780029\n",
      "840\n",
      "current time:- 2023-11-03 03:30:52.724991\n",
      "648 standard\n",
      "379 2297973 completed\n",
      "34.60644578933716\n",
      "648\n",
      "current time:- 2023-11-03 03:31:27.338599\n",
      "687 standard\n",
      "380 2297974 completed\n",
      "35.52414584159851\n",
      "687\n",
      "current time:- 2023-11-03 03:32:02.864992\n",
      "360 standard\n",
      "381 2297975 completed\n",
      "16.37861919403076\n",
      "360\n",
      "current time:- 2023-11-03 03:32:19.248198\n",
      "532 standard\n",
      "382 2298042 completed\n",
      "22.525535106658936\n",
      "532\n",
      "current time:- 2023-11-03 03:32:41.781413\n",
      "1275 too long. Using big context model.\n",
      "383 2298043 completed\n",
      "67.33800077438354\n",
      "1275\n",
      "current time:- 2023-11-03 03:33:49.135940\n",
      "1187 too long. Using big context model.\n",
      "384 2298044 completed\n",
      "59.290578842163086\n",
      "1187\n",
      "current time:- 2023-11-03 03:34:48.431227\n",
      "677 standard\n",
      "385 2298045 completed\n",
      "36.579511880874634\n",
      "677\n",
      "current time:- 2023-11-03 03:35:25.023720\n",
      "598 standard\n",
      "386 2298046 completed\n",
      "28.72144889831543\n",
      "598\n",
      "current time:- 2023-11-03 03:35:53.748647\n",
      "564 standard\n",
      "387 2298047 completed\n",
      "27.77470898628235\n",
      "564\n",
      "current time:- 2023-11-03 03:36:21.530922\n",
      "1284 too long. Using big context model.\n",
      "388 2298048 completed\n",
      "62.0628662109375\n",
      "1284\n",
      "current time:- 2023-11-03 03:37:23.621435\n",
      "413 standard\n",
      "389 2298049 completed\n",
      "19.81178593635559\n",
      "413\n",
      "current time:- 2023-11-03 03:37:43.424241\n",
      "1134 too long. Using big context model.\n",
      "390 2298050 completed\n",
      "68.92692303657532\n",
      "1134\n",
      "current time:- 2023-11-03 03:38:52.355017\n",
      "896 standard\n",
      "391 2298051 completed\n",
      "61.13954496383667\n",
      "896\n",
      "current time:- 2023-11-03 03:39:53.503113\n",
      "1137 too long. Using big context model.\n",
      "392 2298052 completed\n",
      "57.683736085891724\n",
      "1137\n",
      "current time:- 2023-11-03 03:40:51.190840\n",
      "418 standard\n",
      "393 2298053 completed\n",
      "27.64411187171936\n",
      "418\n",
      "current time:- 2023-11-03 03:41:18.840651\n",
      "844 standard\n",
      "394 2298054 completed\n",
      "42.794458866119385\n",
      "844\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "tks=0\n",
    "regex_error=[]\n",
    "re_shard=[]\n",
    "no_ref=[]\n",
    "indivs=[]\n",
    "abnormal=[]\n",
    "apps=[]\n",
    "lg=[]\n",
    "print(str(len(data.keys()))+\" to parse\")\n",
    "for i in data.keys():\n",
    "    res=None\n",
    "    count=count+1\n",
    "    if count<340:\n",
    "        continue\n",
    "    filename=base_path+'/'+i+'_chatgpt.json'\n",
    "    if os.path.exists(filename):\n",
    "        print(str(count)+' '+i+\" completed\")\n",
    "        continue\n",
    "    start=time.time()\n",
    "    \n",
    "    found=0\n",
    "    entry=Merged[Merged[\"ID\"]==i]\n",
    "    if type(data[i])==str:\n",
    "        print(\"pdf not available. download \"+i)\n",
    "        re_shard.append(i)\n",
    "        continue\n",
    "    try:\n",
    "        refs=data[i][\"references\"][0]\n",
    "        if \"found\" in refs.keys():\n",
    "            found=1\n",
    "    except:\n",
    "        print(str(i)+\" has no references, check it\")\n",
    "        print(entry[\"year\"].to_list()[0])\n",
    "        no_ref.append(i)\n",
    "        continue\n",
    "    if found==1:\n",
    "        #response=[]\n",
    "        #print(refs[\"found\"].keys())\n",
    "        #print(refs)\n",
    "        pages=list(refs[\"found\"].keys())\n",
    "        pages.sort()\n",
    "        text=\"\"\n",
    "        for j in pages:\n",
    "            text=text+refs[\"found\"][j][0]+\"\\n\"\n",
    "        position=None\n",
    "        position_a=None\n",
    "        try:\n",
    "            position=regex.search('(^|\\n)R(EFERENCES){e<=3}(\\n| )', text).span(0)[0]\n",
    "        except:\n",
    "            print(\"Regex error: \"+i)\n",
    "            regex_error.append(i)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            position_a=regex.search('(^|\\n)(APPENDIX){e<=3}(\\n| )', text).span(0)[0]\n",
    "            if position < position_a:\n",
    "                apps.append(i)\n",
    "                print(\"ref at \"+str(position)+ \". app at \"+ str(position_a) + \" of \"+str(len(text))+\" in \"+i)\n",
    "            else:\n",
    "                position_a=len(text)\n",
    "        except:\n",
    "            position_a=len(text)\n",
    "        start=time.time()\n",
    "        text=re.sub('\\nThis content downloaded(?s:.*?)jstor.org/terms\\n', \"\", text)\n",
    "        #prompt = 'given the following data. can you please format the references in the text into a .csv format with the following fields: authors, year, month, title, publisher, city, pages, the full reference. Please separate the author names with \";\" as the delimiter if there are multiple authors. Please use quotes around text and \"\" for missing data.\\n'+text[position:]\n",
    "        #prompt = 'given the following data. can you please format the references in the text into a .csv format with the following fields: authors, year, month, title, publisher, pages, the full reference. Please separate the author names with \"and\" as the delimiter if there are multiple authors. Please use quotes around text and \"\" for missing data. Use ; as the delimiter.\\n'+text[position:]\n",
    "        #prompt = 'given the following data. can you please extract and format the references in the text into a json dictionary with the following fields: authors, year, month, title, publisher, city, pages, the full reference. Please separate the author names with \";\" as the delimiter if there are multiple authors. Please use quotes around text and \"\" for missing data.\\n'+text[position:]\n",
    "\n",
    "        #prompt = 'please extract the references in the following data and format it in chicago referencing style.'+text[position:position_a].upper()\n",
    "        #prompt = 'please extract the references in the following data and format it in harvard referencing style.'+text[position:position_a]\n",
    "        #prompt = 'Given the following reference list. Please extract the following fields: authors, year, month, title, publisher, pages, and the full reference in Chicago referencing style. Please separate the author names with \";\" as the delimiter if there are multiple authors. Please \"NA\" as a placeholder for missing data.\\n'+text[position:position_a].upper()\n",
    "        prompt = 'Given the following reference list, please extract the following fields of the reference into a dictionary: authors, year, title, month, publisher, pages, and the full reference in Chicago referencing style. Please separate the author names with \";\" as the delimiter if there are multiple authors. Please use \"NA\" as a placeholder for missing data.\\n'+text[position:position_a].upper()\n",
    "\n",
    "#         print(prompt)\n",
    "        toks=num_tokens_from_messages([{\"role\": \"user\", \"content\": prompt}])\n",
    "        tks=tks+toks\n",
    "        print(\"current time:-\", datetime.datetime.now())\n",
    "\n",
    "        indivs.append(toks)\n",
    "        \n",
    "        if toks>3501:\n",
    "            abnormal.append(i)\n",
    "            print(i+ \" this has an abnormally long reference list at \"+str(toks)+\", process separately\")\n",
    "            continue\n",
    "        try:   \n",
    "            if toks<1100:\n",
    "                print(str(toks)+\" standard\")\n",
    "                res=get_completion(prompt, \"gpt-3.5-turbo\")\n",
    "                if res[\"choices\"][0][\"finish_reason\"]==\"length\":\n",
    "                    print(\"failed to return appropriate length\")\n",
    "                    res=get_completion(prompt, \"gpt-3.5-turbo-16k\")\n",
    "                response[i]=res\n",
    "            else:\n",
    "                print(str(toks)+\" too long. Using big context model.\")\n",
    "                lg.append(i)\n",
    "                res=get_completion(prompt, \"gpt-3.5-turbo-16k\")\n",
    "                response[i]=res\n",
    "\n",
    "            if res!=None:    \n",
    "                with open(filename, 'w') as f:\n",
    "                    json.dump({i:res}, f)\n",
    "                    print(str(count)+' '+i+\" completed\")\n",
    "        except:\n",
    "            print(\"this took too long to respond, complete next round. Moving on!\")\n",
    "                \n",
    "    end=time.time()\n",
    "    print(end-start)\n",
    "    print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b12572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path+'/ECTA_2011_2020_chatgpt_output_max3500.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "0e64d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in response.keys():\n",
    "    temp_dict={i:response[i]}\n",
    "    with open(base_path+'/'+i+'_chatgpt.json', 'w') as f:\n",
    "        json.dump(temp_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "97bf7a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"23357243\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cce156de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in response:\n",
    "    if response[i][\"choices\"][0][\"finish_reason\"]!=\"stop\":\n",
    "        print(i)\n",
    "#         print(response[i][\"choices\"][0]['message']['content'])\n",
    "#     else:\n",
    "#         print(\"**********\")\n",
    "#         print(response[i][\"choices\"][0]['message']['content'][0:200])\n",
    "#         print(\"**********\")\n",
    "#         print(response[i][\"choices\"][0]['message']['content'][-200:])\n",
    "#         print(\"**********\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "f1e09032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.111940298507463\n",
      "4.378571428571429\n",
      "3.5120481927710845\n",
      "3.7745664739884393\n"
     ]
    }
   ],
   "source": [
    "print(3097/134)\n",
    "print(613/140)\n",
    "print(1166/332)\n",
    "print(653/173)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "d877197b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461.1111111111111"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "332*5000/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "4ccbb5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140\n",
      "2786\n",
      "1416\n"
     ]
    }
   ],
   "source": [
    "print(response[15][\"usage\"][\"completion_tokens\"])\n",
    "print(response[16][\"usage\"][\"completion_tokens\"])\n",
    "print(response[17][\"usage\"][\"completion_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "db3636ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.537719298245614\n",
      "0.41852117731514715\n",
      "0.4611581920903955\n"
     ]
    }
   ],
   "source": [
    "print(613/response[15][\"usage\"][\"completion_tokens\"])\n",
    "print(1166/response[16][\"usage\"][\"completion_tokens\"])\n",
    "print(653/response[17][\"usage\"][\"completion_tokens\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lib",
   "language": "python",
   "name": "lib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
