{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8291c91e",
   "metadata": {},
   "source": [
    "# JPE references and affiliations extraction\n",
    "\n",
    "## Table of Contents\n",
    "Check how to make a bloody table of contents for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279de19",
   "metadata": {},
   "source": [
    "## Merged dataset field description\n",
    "\n",
    "This is a description of fields in the Merged dataset that combines JPE masterlist, pivot list, Scopus data pre-2016. The Merged dataset is stored in JPE_M_sco_du.xlsx.\n",
    "\n",
    "    'URL' : JSTOR url for article \n",
    "    'urldate': date \n",
    "    'author' : Author names recorded by JSTOR generally in the form \"last name, first name initial.\" with multiple authors joined by \"and\" or \",\"\n",
    "    'author_split': The previous field split ['author 1', 'author 2', 'author 3'...]\n",
    "    'reviewed-author' : If it is a review this is the field that will record the reviewed author(s)' name\n",
    "    'title' : Title of article recorded by JSTOR citation files\n",
    "    'title10' : This is previously scraped title data from jstor article pages, I've noticed inconsistencies where the review title is missing in the citation file and this can be used as a supplement.\n",
    "    'abstract' : abstract recorded by JSTOR nb: this is not consistent\n",
    "    'content_type' : Article type determined during cleaning. Includes MISC for miscellaneous, Reviews, Note, Comment, Rejoinder and Article categorizations\n",
    "    'issue_url' : url of issue article belongs to on JSTOR, if this is from the original sources then \n",
    "    'pages' : pages as recorded by JSTOR\n",
    "    'year' : Year of publication recorded by JSTOR\n",
    "    'volume' : Volume of article recorded by JSTOR\n",
    "    'number' : issue of article recorded by JSTOR\n",
    "    'ISSN' : CHECK THIS, from JSTOR\n",
    "    'journal' : journal name JSTOR\n",
    "    'type' : Type of issue determined during cleaning. S for special issue. N for normal issue\n",
    "    'authorsSCO' : Author names recorded by Scopus\n",
    "    'titleSCO' : Title recorded by Scopus\n",
    "    'journalSCO' : Journal name recorded by Scopus\n",
    "    'DOI' : DOI recorded by scopus\n",
    "    'affiliations' : affiliations of authors as recorded by scopus\n",
    "    'abstractSCO' : abstract of article recorded by scopus\n",
    "    'citations' : citations of article recorded by scopus\n",
    "    'document type' : Article type recorded by scopus, may differ from that in cleaning\n",
    "    'index keywords' : from scopus\n",
    "    'author keywords' : from scopus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654833cb",
   "metadata": {},
   "source": [
    "## The Tesseract library\n",
    "\n",
    "I use Tesseract, a popular parsing library and the python binding of it pyTesseract to parse JPE documents in this section. I am  following the code and technique from this article on how to read a multi-column pdf. The fitz python module is a lightweight pdf reader, it will require the installation of pyMupdf. OpenCV python module and the module/class cv2 from it is used to apply otsu's thresholding technique to lift/determine paragraph edges.\n",
    "\n",
    "https://towardsdatascience.com/read-a-multi-column-pdf-with-pytesseract-in-python-1d99015f887a\n",
    "\n",
    "There are some configurations of tesseract that need to be set to apply it effectively. These are\n",
    "\n",
    "Page segmentation modes: where tesseract automatically performs some sort of otsu thresholding to determine blobs of text to ensure the correct output order. There are 12 modes, by default tesseract is mode 3 which does page sengmentation. The two I will switch between is 3 (great for automatically detecting column data) and 6 (for text in a single column). Although mode 3 can achieve the similar results as 6, mode 3 looks for vertical blank spacing between columns to determine separate bodies of text. If something is in a listed format, mode 3 will assume that the list numerics or points are a separate column and read the text from left column to right column.\n",
    "\n",
    "OEM: (need to check the full def of this too) the engine used to ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bc12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# for OCR using PyTesseract\n",
    "import cv2                              # pre-processing images\n",
    "import pytesseract                      # extracting text from images\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt         # displaying output images\n",
    "from PIL import Image\n",
    "import regex\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483f3dc",
   "metadata": {},
   "source": [
    "Set path to where pdfs of articles are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf4565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"/Users/sijiawu/Work/Refs Danae/Thesis/Data\"\n",
    "temp=base_path+'/PDFs/JPE/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c939021c",
   "metadata": {},
   "source": [
    "Read in the merged pdf containing jstor, scopus and datadump metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d91b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged=pd.read_excel(base_path+'/Combined/JPE_M_sco_du.xlsx')\n",
    "Merged.loc[Merged['journal']==\"Journal of Political Economy\",'journal']='JPE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d900c0f",
   "metadata": {},
   "source": [
    "Set the zoom factor to zoom into the pdf. This is to get a higher resolution image. I have chosen 2x zoom for both vertically and horizontally. This doesn't matter if we use the pngs that were previously generated via the split_pdf script in part two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd111ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_x = 2.0 # horizontal zoom\n",
    "zoom_y = 2.0 # vertical zoom\n",
    "mat = fitz.Matrix(zoom_x, zoom_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c888964",
   "metadata": {},
   "source": [
    "## EDA\n",
    "I am printing the first and last page of each paper within a range(s) of years. This is a quick way to look at the article content in the data set to observe and identify any changes in the the layout and positioning of affiliations or references. Specifically, this enables me to:\n",
    "1. Apply the most appropriate page segmentation mode ie: 3 or 6 for when the formatting changes in which range of years\n",
    "2. Identify the year at which to stop looking for references for when citations shift to the footnotes\n",
    "3. Identify strange cases where the start of an article is on the same page as the end of the previous article. The risk here is if the following article does not have references, my script may erroneously attribute the previous articles reference section to the following article. These cases should be minimal, so excluding them will be simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad805d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "investigate=Merged[(Merged.year<=1967)&(Merged.year>=1960)&(Merged.content_type!=\"MISC\")&(Merged.content_type!=\"Review\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5645834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing first and last page, the assumption is these pngs already exist from the previous stage\n",
    "# this for loop may also be modified to print every page\n",
    "for i in investigate.index:\n",
    "    filepath=base_path+'/PDFs/JPE/'+Merged.iloc[i]['URL'].split('/')[-1]+'.pdf'\n",
    "    if os.path.exists(filepath):\n",
    "        doc=fitz.open(filepath)\n",
    "        print(Merged.iloc[i]['year'])\n",
    "        print(Merged.iloc[i]['number'])\n",
    "        print(Merged.iloc[i]['volume'])\n",
    "        print(Merged.iloc[i]['author'])\n",
    "        print(Merged.iloc[i]['title'])\n",
    "#         for page in doc: \n",
    "            #if (page.number == 1) or (page.number==(doc.page_count-1)):\n",
    "        png = base_path+\"/PDFs/JPE/png/\" + Merged.iloc[i]['URL'].split('/')[-1].split('.')[0] + '_wo_cover_page-0.png'\n",
    "        png2 = base_path+\"/PDFs/JPE/png/\" + Merged.iloc[i]['URL'].split('/')[-1].split('.')[0] + '_wo_cover_page-'+str(doc.page_count-2)+'.png'\n",
    "        print(png)\n",
    "        print(png2)\n",
    "        if (os.path.exists(png)==True)&(os.path.exists(png2)==True):\n",
    "            original_image = cv2.imread(png)\n",
    "\n",
    "            # convert the image to grayscale\n",
    "            gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            plt.imshow(gray_image, cmap='gray')\n",
    "            plt.show()\n",
    "            \n",
    "            original_image = cv2.imread(png2)\n",
    "\n",
    "            # convert the image to grayscale\n",
    "            gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            plt.imshow(gray_image, cmap='gray')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab70fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=['1832163'] #this has the problem of point 3\n",
    "separate=['1828842'] # this has a reference in it's footnotes but no reference, however the articles in the rest of the issue have a reference list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b255395",
   "metadata": {},
   "source": [
    "### EDA Results\n",
    "\n",
    "I will proceed by processing the data by decades or less if there are changes mid-way. Generally, I expect conventions to be consistent within a single issue (ie: reference list or no reference list) but this can be inconsistent.\n",
    "\n",
    "From 1971 to 2020 (inclusive), articles are in single column format with references at the end.\n",
    "\n",
    "From 1968 to 1970 (inclusive), articles are in double column format and each article starts on a new page regardless of what type it is. No need to make special provisions to identify the start and end of an article.\n",
    "\n",
    "From 1966 to 1967 (inclusive), articles are in double column format, and non-articles such as reviews, comments etc. begin on the same page as the previous article ends. The only article to pose an issue is 1832163, because the previous article's reference list is on the first page of it and the article itself does not have any references. The exception is 1828842 that has references in its footnotes contrary to the rest of the articles in the issue.\n",
    "\n",
    "From 1940 to 1965 (inclusive), articles are in double column format and they do not have reference lists at the end. Citations are found in the footnotes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeff039",
   "metadata": {},
   "source": [
    "### The converter() function\n",
    "This function takes a string and replaces all non-ascii characters with a placeholder. In regex, a placeholder is represented by a '.'. Specific to JPE, a lower case 'L' and a upper case 'I' look the same to tesseract because of the font. Hence, upper case 'I's are replaced with a placeholder. I found that for middle name initials, tesseract may mistake the letter for something else  hence again, we replace it with a placeholder. \n",
    "\n",
    "An alternative solution is to use fuzzy matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e86eb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "def converter(teststring):\n",
    "    for i in range(len(teststring)):\n",
    "        if teststring[i] == '.':\n",
    "            teststring=teststring[0:i-1]+'.'+teststring[i:]\n",
    "    \n",
    "    teststring=teststring.replace('I','.')\n",
    "            \n",
    "    for i in teststring:\n",
    "        if (i not in string.ascii_lowercase) & (i not in string.ascii_uppercase) & (i !=' '):\n",
    "            teststring=teststring.replace(i,'.')\n",
    "    return teststring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b0f94",
   "metadata": {},
   "source": [
    "### The generate_pngs() function\n",
    "This function looks for the block of text that contains author names, assuming that the block also contains affiliations. If affiliations are not found, then the parsed text is returned. Only the first page of the article and sometimes the JSTOR cover page is parsed. \n",
    "\n",
    "Given a pdf file path (SCANNED_FILE), the number of pages (pages), zoom matrix (mat), path to pdf file folder (path), a value for how tightly to draw mask (k_val). A higher k_val results in a mask that covers more of the page ie: segments the page less. You can uncomment the lines of code for plots inside the function observe the mask. Lastly, a string or regex pattern that would match the lead author's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f530213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pngs(SCANNED_FILE, pages, mat, path, k_val, author):\n",
    "    doc = fitz.open(SCANNED_FILE)\n",
    "    parsed={}\n",
    "    count=doc.page_count-pages\n",
    "    if count<0:\n",
    "        count=1\n",
    "    for page in doc:\n",
    "        if (page.number == count):\n",
    "            png = path+\"\\\\pages_png\\\\\" + SCANNED_FILE.split('\\\\')[-1].split('.')[0] + '_page-%i.png' % page.number\n",
    "            if os.path.exists(png)==False:\n",
    "                pix = page.get_pixmap(matrix=mat)\n",
    "                print(png)\n",
    "                pix.save(png)\n",
    "\n",
    "            parsed[page.number]=[]\n",
    "\n",
    "            original_image = cv2.imread(png)\n",
    "            # convert the image to grayscale\n",
    "            gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            #plt.figure(figsize=(25, 15))\n",
    "            #plt.imshow(gray_image, cmap='gray')\n",
    "            #plt.show()\n",
    "\n",
    "            # Performing OTSU threshold\n",
    "            ret, threshold_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n",
    "\n",
    "            #plt.figure(figsize=(25, 15))\n",
    "            #plt.imshow(threshold_image, cmap='gray')\n",
    "            #plt.show()\n",
    "\n",
    "            rectangular_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (k_val, k_val))\n",
    "\n",
    "            # Applying dilation on the threshold image\n",
    "            dilated_image = cv2.dilate(threshold_image, rectangular_kernel, iterations = 1)\n",
    "\n",
    "            #plt.figure(figsize=(25, 15))\n",
    "            #plt.imshow(dilated_image)\n",
    "            #plt.show()\n",
    "\n",
    "            # Finding contours\n",
    "            contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # Creating a copy of the image\n",
    "            copied_image = original_image.copy()\n",
    "\n",
    "            mask = np.zeros(original_image.shape, np.uint8)\n",
    "            i=1\n",
    "            # Looping through the identified contours\n",
    "            # Then rectangular part is cropped and passed on to pytesseract\n",
    "            # pytesseract extracts the text inside each contours\n",
    "            # Extracted text is then written into a text file\n",
    "            for cnt in reversed(contours):\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                print(i)\n",
    "                # Cropping the text block for giving input to OCR\n",
    "                cropped = copied_image[y:y + h, x:x + w]\n",
    "                # Apply OCR on the cropped image\n",
    "                text = pytesseract.image_to_string(cropped, lang='lat', config='--oem 3 --psm 1')\n",
    "                print(text)\n",
    "                parsed[page.number].append(text)\n",
    "                print(re.search(author.upper(),text.upper()))\n",
    "                if re.search('AUTHOR\\(S\\)', text.upper()) is not None:\n",
    "                    count+=1\n",
    "                    break\n",
    "                if re.search(author.upper(),text.upper()) is not None:\n",
    "                    return {'found': text}\n",
    "                #masked = cv2.drawContours(mask, [cnt], 0, (255, 255, 255), -1)\n",
    "                print()\n",
    "                i=i+1\n",
    "            #plt.figure(figsize=(25, 15))\n",
    "            #plt.imshow(masked, cmap='gray')\n",
    "            #plt.show()\n",
    "    return {'raw': parsed}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76abf022",
   "metadata": {},
   "source": [
    "### Testing the generate_pngs() function\n",
    "I use the getNumberofPages() and converter() functions as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6db6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with you own file\n",
    "SCANNED_FILE = path+'\\\\1830926.pdf'\n",
    "\n",
    "t0=time.time()        \n",
    "affiliations=generate_pngs(SCANNED_FILE, getNumberofPages('339-354'), mat, path, 50, converter('Michael D. Intriligator'))\n",
    "t1=time.time()\n",
    "total=t1-t0\n",
    "print(total)\n",
    "affiliations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98573513",
   "metadata": {},
   "source": [
    "### Extracting affiliations from JPE\n",
    "First create an empty dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5473222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4430, 25)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict={}\n",
    "\n",
    "#lower case all letters in both upper and lower\n",
    "counts=Merged[(Merged['year']>1940) & (Merged['content_type']!='MISC') & (Merged['content_type']!='Review')]\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8454ab",
   "metadata": {},
   "source": [
    "This for loop, provided the content_type is not miscellaneous or a review, stores metadata of a paper via the JSTOR ID in the dictionary dict.\n",
    "\n",
    "JSTOR_id: { \n",
    "\n",
    "    'affiliations': {'found': affiliations_text_if_found}, \n",
    "    'content_type': content_type, \n",
    "    'authors': [author1, author2, author3 ...], \n",
    "    'stable_url': stable_url]\n",
    "   }\n",
    "   \n",
    "Note: if affiliations are not found then the 'affiliations' field will contain a dictionary of form.\n",
    "\n",
    "'raw': {\n",
    "\n",
    "    '0': [parsed_text_on_page_0 separated by commas], \n",
    "    '1': [parsed_text_on_page_1 separated by commas] ...\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9605a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time.time()\n",
    "\n",
    "for i in Merged[(Merged['year']>=1940) & (Merged['content_type']!='MISC') & (Merged['content_type']!='Review')].index:\n",
    "    if Merged.iloc[i]['Jstor_authors'] is not NaN: \n",
    "        if \"Suggested by\" not in Merged.iloc[i]['Jstor_authors']:\n",
    "            authors=str(Merged.iloc[i]['Jstor_authors']).replace(' and ',', ').replace(\"  \",' ').split(',')\n",
    "            filepath=path+'\\\\'+Merged.iloc[i]['stable_url'].split('/')[-1]+'.pdf'\n",
    "            if os.path.exists(filepath)==True:\n",
    "                print(Merged.iloc[i]['year'])\n",
    "                first_author=converter(authors[0])\n",
    "                print(first_author)\n",
    "                n_pages=getNumberofPages(Merged.iloc[i]['pages'])\n",
    "                if pd.isna(n_pages)==False:\n",
    "                    affiliations=generate_pngs(filepath, n_pages, mat, path, 52, first_author.strip())\n",
    "                    dict[Merged.iloc[i]['stable_url'].split('/')[-1]]={'affiliations':affiliations, 'content_type':Merged.iloc[i]['content_type'], 'authors':authors, 'stable_url': Merged.iloc[i]['stable_url']}\n",
    "            else:\n",
    "                dict[Merged.iloc[i]['stable_url'].split('/')[-1]]='PDF not available, download at '+ Merged.iloc[i]['stable_url']\n",
    "t1=time.time()\n",
    "total=t1-t0\n",
    "print(total)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d576a5",
   "metadata": {},
   "source": [
    "Save the dictionary containing affiliations inside a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "406259fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(path+'//JPE_affiliation_output_aff2.json','w') as fp:\n",
    "    json.dump(dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# print pretty to view dictionary content\n",
    "print(json.dumps(dict, sort_keys=False, indent=4))\n",
    "print(len(dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e68b6f",
   "metadata": {},
   "source": [
    "### Extracting references\n",
    "JPE has references at the end in a dedicated references section from 1966 onwards. Hence the generate_refs function looks for a keyword 'References' using fuzzy matching (less that 3 character difference) and returns everything following it. If it is not found then the function returns the parsed text of the full page of the document.\n",
    "\n",
    "TODO: MODIFICATION REQ TO CHANGE THIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8bdef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version ignores contouring and sectioning out paragraphs. It directly feeds the image to tesseract. \n",
    "# There seems to be no image resolution degradation this way as opposed to reduced using the openCV library.\n",
    "# SCANNED_FILE: is for the full path to the original pdf. we require this to get the number of pages. \n",
    "#  The assumption is that the jstor (or other) cover page has been removed previously, so in our case it will always have wo_cover.pdf as suffix.\n",
    "# path: to the folder containing pre-generated pngs, the pngs in this folder are assumed to have the same file name as the SCANNED_FILE + suffix page-{page no}.png for each sharded page\n",
    "# keyword: this is whatever regex pattern that you wish to search for. This function uses the regex.search method from the regex library\n",
    "#  It can take fuzzy match regex patterns\n",
    "# config: this is the tesseract configuration default is '--oem 1 --psm 3', which is also the default for this function\n",
    "#  3 implies automatic page segmentation, better for 2 column format pdfs, 6 assumes single column, top to bottom text and will preserve each line ending better.\n",
    "def generate_refs2(SCANNED_FILE, path, keyword, custom_config = r'--oem 1 --psm 3'):\n",
    "    try:\n",
    "        doc = fitz.open(SCANNED_FILE)\n",
    "    except:\n",
    "        print(\"could not open: \"+SCANNED_FILE)\n",
    "        raise Exception(\"this file is corrupt\")\n",
    "    if \"wo_cover\" not in SCANNED_FILE:\n",
    "        print(\"warning, the file: \"+SCANNED_FILE.split('/')[-1]+\" does not have it's coverpage removed.\\nThis function will continue. Assumed image file name convention is: \"+SCANNED_FILE.split('/')[-1].split('.')[0] + '_page-{number}.png')\n",
    "    parsed={}\n",
    "    references={}\n",
    "    found=0\n",
    "    for page in reversed(doc):\n",
    "        png = path+\"/\" + SCANNED_FILE.split('/')[-1].split('.')[0] + '_page-%i.png' % page.number\n",
    "#             print(png)\n",
    "        parsed[page.number]=[]\n",
    "        references[page.number]=[]\n",
    "        if os.path.exists(png)==True:\n",
    "            text = pytesseract.image_to_string(png, config=custom_config)\n",
    "#                 print(text)\n",
    "            parsed[page.number].append(text)\n",
    "            keyword_search=regex.search(keyword,text.upper())\n",
    "            if keyword_search is not None:\n",
    "                print('found')\n",
    "                return {'found': parsed, \"pages\":doc.page_count}\n",
    "        else:\n",
    "            print(\"error: this image does not exist, please generate png shards at 300 dpi in path: \"+path)\n",
    "    print(\"the keyword: \"+keyword + \"was not found. But this is the full tesseract output nonetheless.\")\n",
    "    return {'raw': parsed, \"pages\": doc.page_count}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b7173",
   "metadata": {},
   "source": [
    "### Testing the generate_pngs() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "622b2fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n"
     ]
    }
   ],
   "source": [
    "# 1832164_wo_cover_page\n",
    "filepath=temp+'wo_cover/'+'1829157_wo_cover.pdf'\n",
    "op=generate_refs2(filepath, temp+\"png\", '(^|\\n)R(EFERENCES){e<=3}(\\n| )', r'--oem 1 --psm 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed0870",
   "metadata": {},
   "source": [
    "Create the function to run each article through the generate_refs2 function. Each file is saved individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time.time()\n",
    "custom_config = r'--oem 1 --psm 3'\n",
    "\n",
    "for i in Merged[(Merged['year']<=1967) & (Merged['year']>1965)& (Merged['content_type']!='MISC') & (Merged['content_type']!='Review')].index:\n",
    "    filepath=temp+'wo_cover/'+Merged.iloc[i]['URL'].split('/')[-1]+'_wo_cover.pdf'\n",
    "    print(filepath)\n",
    "     if os.path.exists(filepath)==True:\n",
    "        references=generate_refs2(filepath, temp+\"png\", '(^|\\n)R(EFERENCES){e<=3}(\\n| )', custom_config)\n",
    "        o_file=base_path+'/'+Merged.iloc[i]['URL'].split('/')[-1]+ \"_tesseract.json\"\n",
    "        with open(o_file,'w') as f:\n",
    "                    json.dump({Merged.iloc[i]['URL'].split('/')[-1]: {'references':references, 'URL': Merged.iloc[i]['URL']}}, f, indent=3)\n",
    "    else:\n",
    "        dict_ref[Merged.iloc[i]['URL'].split('/')[-1]]='PDF not available, download at '+ Merged.iloc[i]['URL']\n",
    "        print(\"filepath not valid, file \"+Merged.iloc[i]['URL'].split('/')[-1]+'_wo_cover.pdf'+ \" did not get sharded\")\n",
    "t1=time.time()\n",
    "total=t1-t0\n",
    "print(total)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731da8a",
   "metadata": {},
   "source": [
    "This for loop, provided the content_type is not miscellaneous or a review, stores the references extracted via tesseract and metadata of a paper via the JSTOR ID in the dictionary dict.\n",
    "\n",
    "JSTOR_id: {\n",
    "\n",
    "    'references': {\n",
    "        'found': {\n",
    "            'page_no': [Text containing or following references keyword separated by commas],\n",
    "            'page_no': [Text containing or following references keyword separated by commas] ...\n",
    "            }\n",
    "         }, \n",
    "    'content_type': content_type, \n",
    "    'authors': [author1, author2, author3 ...], \n",
    "    'stable_url': stable_url]\n",
    "    }\n",
    "\n",
    "Note: if references are not found then the 'references' field will contain a dictionary of form.\n",
    "\n",
    "'raw': {\n",
    "\n",
    "    'page_no': [parsed_text_on_page_no separated by commas], \n",
    "    'page_no': [parsed_text_on_page_no separated by commas] ...\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3384efa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e4b711c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lib",
   "language": "python",
   "name": "lib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
