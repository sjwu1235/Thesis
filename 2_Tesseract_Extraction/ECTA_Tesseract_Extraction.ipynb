{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7965c8",
   "metadata": {},
   "source": [
    "# ECTA Tesseract Extraction\n",
    "\n",
    "This notebook extracts references and affiliations using tesseract\n",
    "\n",
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20247487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# for OCR using PyTesseract\n",
    "import cv2                              # pre-processing images\n",
    "import pytesseract                      # extracting text from images\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt         # displaying output images\n",
    "from PIL import Image\n",
    "import regex\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96051e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"/Users/sijiawu/Work/Refs Danae/Thesis/Data\"\n",
    "temp=base_path+'/PDFs/ECTA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9505e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged=pd.read_excel(base_path+'/Combined/ECTA_M_sco_du.xlsx')\n",
    "Merged.loc[Merged['journal']==\"Econometrica\",'journal']='ECTA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5d4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "investigate=Merged[(Merged.year<=1960)&(Merged.year>=1950)&(Merged.content_type!=\"MISC\")&(Merged.content_type!=\"Review\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af5c5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude={\n",
    "    1971: \"4\",\n",
    "    1970: \"4\",\n",
    "    1968: \"5\",\n",
    "    1967: \"5\",\n",
    "    1966: \"5\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f896f28",
   "metadata": {},
   "source": [
    "* 1971 issue 4 \n",
    "* 1970 issue 4\n",
    "* 1968 issue 5\n",
    "* 1967 issue 5\n",
    "* 1966 issue 5\n",
    "\n",
    "These are special issues with no author names, it is actually a list of abstracts where the title of each \"article\" is the title of the article set or discussion panel. Some have references but if these are abstracts then the papers are likely published elsewhere. I choose to exclude this issue due to the previous statement and because this is too inconsistent with the rest of ECTA to process with tesseract. You may be able to ask chatgpt to parse this entirely as a full issue. But this is for last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfcada",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# printing first and last page, the assumption is these pngs already exist from the previous stage\n",
    "# this for loop may also be modified to print every page\n",
    "number=None\n",
    "count=0\n",
    "pgs=0\n",
    "for i in investigate.index:\n",
    "#     if number == None:\n",
    "#         number = Merged.iloc[i]['number']\n",
    "#         count=count+1\n",
    "#         continue\n",
    "#     elif (str(number)!=str(Merged.iloc[i]['number'])):\n",
    "#         number=str(Merged.iloc[i]['number'])\n",
    "#         count=1\n",
    "#         continue\n",
    "#     elif (str(number)==str(Merged.iloc[i]['number']))&(count<=5):\n",
    "#         count=count+1\n",
    "#         continue\n",
    "#     else:\n",
    "#         count=count+1\n",
    "    filepath=base_path+'/PDFs/ECTA/'+Merged.iloc[i]['URL'].split('/')[-1]+'.pdf'\n",
    "    if os.path.exists(filepath):\n",
    "        doc=fitz.open(filepath)\n",
    "        print(Merged.iloc[i]['year'])\n",
    "        print(Merged.iloc[i]['number'])\n",
    "        print(Merged.iloc[i]['volume'])\n",
    "        print(Merged.iloc[i]['author'])\n",
    "        print(Merged.iloc[i]['title'])\n",
    "        pgs=pgs+doc.page_count\n",
    "#         for page in doc: \n",
    "#             if (page.number == 1) or (page.number==(doc.page_count-1)):\n",
    "            \n",
    "        png = base_path+\"/PDFs/ECTA/png/\" + Merged.iloc[i]['URL'].split('/')[-1].split('.')[0] + '_wo_cover_page-0.png'\n",
    "        png2 = base_path+\"/PDFs/ECTA/png/\" + Merged.iloc[i]['URL'].split('/')[-1].split('.')[0] + '_wo_cover_page-'+str(doc.page_count-2)+'.png'\n",
    "        print(png)\n",
    "        #print(png2)\n",
    "        if (os.path.exists(png)==True)&(os.path.exists(png2)==True):\n",
    "            original_image = cv2.imread(png)\n",
    "\n",
    "            # convert the image to grayscale\n",
    "            gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            plt.imshow(gray_image, cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "            #uncomment to print last page too\n",
    "            original_image = cv2.imread(png2)\n",
    "\n",
    "            # convert the image to grayscale\n",
    "            gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            plt.imshow(gray_image, cmap='gray')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"sorry, but this \"+filepath+\" was not sharded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ac83c",
   "metadata": {},
   "source": [
    "### Extracting References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f324417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version ignores contouring and sectioning out paragraphs. It directly feeds the image to tesseract. \n",
    "# There seems to be no image resolution degradation this way as opposed to reduced using the openCV library.\n",
    "# SCANNED_FILE: is for the full path to the original pdf. we require this to get the number of pages. \n",
    "#  The assumption is that the jstor (or other) cover page has been removed previously, so in our case it will always have wo_cover.pdf as suffix.\n",
    "# path: to the folder containing pre-generated pngs, the pngs in this folder are assumed to have the same file name as the SCANNED_FILE + suffix page-{page no}.png for each sharded page\n",
    "# keyword: this is whatever regex pattern that you wish to search for. This function uses the regex.search method from the regex library\n",
    "#  It can take fuzzy match regex patterns\n",
    "# config: this is the tesseract configuration default is '--oem 1 --psm 3', which is also the default for this function\n",
    "#  3 implies automatic page segmentation, better for 2 column format pdfs, 6 assumes single column, top to bottom text and will preserve each line ending better.\n",
    "def generate_refs2(SCANNED_FILE, path, keyword, custom_config = r'--oem 1 --psm 3'):\n",
    "    try:\n",
    "        doc = fitz.open(SCANNED_FILE)\n",
    "    except:\n",
    "        print(\"could not open: \"+SCANNED_FILE)\n",
    "        raise Exception(\"this file is corrupt\")\n",
    "    if \"wo_cover\" not in SCANNED_FILE:\n",
    "        print(\"warning, the file: \"+SCANNED_FILE.split('/')[-1]+\" does not have it's coverpage removed.\\nThis function will continue. Assumed image file name convention is: \"+SCANNED_FILE.split('/')[-1].split('.')[0] + '_page-{number}.png')\n",
    "    parsed={}\n",
    "    references={}\n",
    "    found=0\n",
    "    for page in reversed(doc):\n",
    "        png = path+\"/\" + SCANNED_FILE.split('/')[-1].split('.')[0] + '_page-%i.png' % page.number\n",
    "#             print(png)\n",
    "        parsed[page.number]=[]\n",
    "        references[page.number]=[]\n",
    "        if os.path.exists(png)==True:\n",
    "            text = pytesseract.image_to_string(png, config=custom_config)\n",
    "#                 print(text)\n",
    "            parsed[page.number].append(text)\n",
    "            keyword_search=regex.search(keyword,text.upper())\n",
    "            if keyword_search is not None:\n",
    "                print('found')\n",
    "                return {'found': parsed, \"pages\":doc.page_count}\n",
    "        else:\n",
    "            print(\"error: this image does not exist, please generate png shards at 300 dpi in path: \"+path)\n",
    "    print(\"the keyword: \"+keyword + \"was not found. But this is the full tesseract output nonetheless.\")\n",
    "    return {'raw': parsed, \"pages\": doc.page_count}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c19865",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ddcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee051025",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time.time()\n",
    "custom_config = r'--oem 1 --psm 6'\n",
    "\n",
    "for i in Merged[(Merged['year']<=2020) & (Merged['year']>=1940)& (Merged['content_type']!='MISC') & (Merged['content_type']!='Review')].index:\n",
    "    if Merged.iloc[i]['year'] in exclude.keys():\n",
    "        if (exclude[Merged.iloc[i]['year']]==Merged.iloc[i][\"number\"]):\n",
    "            #this is the special issue we are excluding\n",
    "            continue\n",
    "    #construct file name\n",
    "    filepath=temp+'wo_cover/'+Merged.iloc[i]['URL'].split('/')[-1]+'_wo_cover.pdf'\n",
    "    print(filepath)\n",
    "    if os.path.exists(filepath)==True:\n",
    "        references=generate_refs2(filepath, temp+\"png\", '(^|\\n)R(EFERENCES){e<=3}(\\n| )', custom_config)\n",
    "        o_file=base_path+'/'+Merged.iloc[i]['URL'].split('/')[-1]+ \"_tesseract.json\"\n",
    "        with open(o_file,'w') as f:\n",
    "                    json.dump({Merged.iloc[i]['URL'].split('/')[-1]: {'references':references, 'URL': Merged.iloc[i]['URL']}}, f, indent=3)\n",
    "    else:\n",
    "        dict_ref[Merged.iloc[i]['URL'].split('/')[-1]]='PDF not available, download at '+ Merged.iloc[i]['URL']\n",
    "        print(\"filepath not valid, file \"+Merged.iloc[i]['URL'].split('/')[-1]+'_wo_cover.pdf'+ \" did not get sharded\")\n",
    "t1=time.time()\n",
    "total=t1-t0\n",
    "print(total)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f646cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa297c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efca92cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df99a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lib",
   "language": "python",
   "name": "lib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
